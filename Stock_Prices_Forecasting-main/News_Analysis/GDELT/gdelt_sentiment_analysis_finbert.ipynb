{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d37a8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9b64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../data/gdelt_exports_cache_concat\"\n",
    "output_dir = \"../data/gdelt_exports_cache_sentiment\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load FinBERT model\n",
    "MODEL_NAME = 'yiyanghkust/finbert-tone'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "finbert = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "# Keywords or NASDAQ-100 company names (you can expand this list!)\n",
    "nasdaq_keywords = [\n",
    "    \"AAPL\", \"Apple\",\n",
    "    \"MSFT\", \"Microsoft\",\n",
    "    \"AMZN\", \"Amazon.com\", \"Amazon\",\n",
    "    \"NVDA\", \"NVIDIA Corporation\", \"Nvidia\",\n",
    "    \"TSLA\", \"Tesla\",\n",
    "    \"GOOGL\", \"GOOG\", \"Alphabet Inc.\", \"Google\", \"Alphabet\",\n",
    "    \"META\", \"Meta Platforms\", \"Meta\",\n",
    "    \"AVGO\", \"Broadcom Inc.\", \"Broadcom\",\n",
    "    \"COST\", \"Costco Wholesale Corporation\", \"Costco\",\n",
    "    \"PEP\", \"PepsiCo, Inc.\", \"Pepsi\",\n",
    "    \"ADBE\", \"Adobe Inc.\", \"Adobe\",\n",
    "    \"INTC\", \"Intel Corporation\", \"Intel\",\n",
    "    \"NFLX\", \"Netflix, Inc.\", \"Netflix\",\n",
    "    \"NDX\", \"NASDAQ\",\n",
    "    \"BNB\", \"Airbnb, Inc.\", \"Airbnb\",\n",
    "    \"ADI\", \"Analog Devices, Inc.\", \"Analog Devices\",\n",
    "    \"ADP\", \"Automatic Data Processing\", \"Automatic Data Processing, Inc.\",\n",
    "    \"ADSK\", \"Autodesk, Inc.\", \"Autodesk\",\n",
    "    \"AEP\", \"American Electric Power Company, Inc.\", \"American Electric Power\",\n",
    "    \"AMAT\", \"Applied Materials, Inc.\", \"Applied Materials\",\n",
    "    \"AMD\", \"Advanced Micro Devices, Inc.\", \"AMD\", \"Advanced Micro Devices\",\n",
    "    \"AMGN\", \"Amgen Inc.\", \"Amgen\",\n",
    "    \"ANSS\", \"ANSYS, Inc.\", \"ANSYS\",\n",
    "    \"APP\", \"Applovin Corporation\",\n",
    "    \"ARM\", \"Arm Holdings plc\", \"Arm Holdings\",\n",
    "    \"ASML\", \"ASML Holding N.V.\", \"ASML\",\"ASML Holding\",\n",
    "    \"AXON\", \"Axon Enterprise, Inc.\", \"Axon\",\"Axon Enterprise\",\n",
    "    \"AZN\", \"AstraZeneca PLC\", \"AstraZeneca\",\n",
    "    \"BIIB\", \"Biogen Inc.\", \"Biogen\",\n",
    "    \"BKNG\", \"Booking Holdings Inc.\", \"Booking Holdings\",\n",
    "    \"BKR\", \"Baker Hughes Company\", \"Baker Hughes\",\n",
    "    \"CCEP\", \"Coca-Cola Europacific Partners plc\", \"Coca-Cola Europacific Partners\",\n",
    "    \"CDNS\", \"Cadence Design Systems, Inc.\", \"Cadence Design Systems\",\n",
    "    \"CDW\", \"CDW Corporation\",\n",
    "    \"CEG\", \"Constellation Energy Corporation\",\n",
    "    \"CHTR\", \"Charter Communications, Inc.\", \"Charter Communications\",\n",
    "    \"CMCSA\", \"Comcast Corporation\",\n",
    "    \"CPRT\", \"Copart, Inc.\", \"Copart\",\n",
    "    \"CRWD\", \"CrowdStrike Holdings, Inc.\", \"CrowdStrike\",\n",
    "    \"CSCO\", \"Cisco Systems, Inc.\", \"Cisco\",\n",
    "    \"CSGP\", \"CoStar Group, Inc.\", \"CoStar Group\",\n",
    "    \"CSX\", \"CSX Corporation\",\n",
    "    \"CTAS\", \"Cintas Corporation\",\n",
    "    \"CTSH\", \"Cognizant Technology Solutions Corporation\",\n",
    "    \"DASH\", \"DoorDash, Inc.\", \"DoorDash\",\n",
    "    \"DDOG\", \"Datadog, Inc.\", \"Datadog\",\n",
    "    \"DXCM\", \"DexCom, Inc.\", \"DexCom\",\n",
    "    \"EA\", \"Electronic Arts Inc.\", \"Electronic Arts\",\n",
    "    \"EXC\", \"Exelon Corporation\",\n",
    "    \"FANG\", \"Diamondback Energy, Inc.\", \"Diamondback Energy\",\n",
    "    \"FAST\", \"Fastenal Company\", \"Fastenal\"\n",
    "    \"FTNT\", \"Fortinet, Inc.\", \"Fortinet\",\n",
    "    \"GEHC\", \"GE HealthCare Technologies Inc.\", \"GE HealthCare\",\n",
    "    \"GFS\", \"GlobalFoundries Inc.\", \"GlobalFoundries\",\n",
    "    \"GILD\", \"Gilead Sciences, Inc.\", \"Gilead Sciences\",\n",
    "    \"HON\", \"Honeywell International Inc.\", \"Honeywell\",\n",
    "    \"IDXX\", \"IDEXX Laboratories, Inc.\", \"IDEXX Laboratories\",\n",
    "    \"INTU\", \"Intuit Inc.\", \"Intuit\",\n",
    "    \"ISRG\", \"Intuitive Surgical, Inc.\", \"Intuitive Surgical\",\n",
    "    \"KDP\", \"Keurig Dr Pepper Inc.\", \"Keurig Dr Pepper\",\n",
    "    \"KHC\", \"The Kraft Heinz Company\", \"The Kraft Heinz\",\n",
    "    \"KLAC\", \"KLA Corporation\",\n",
    "    \"LIN\", \"Linde plc\",\n",
    "    \"LRCX\", \"Lam Research Corporation\",\n",
    "    \"LULU\", \"lululemon athletica Inc.\", \"lululemon\",\n",
    "    \"MAR\", \"Marriott International\",\n",
    "    \"MCHP\", \"Microchip Technology Incorporated\",\n",
    "    \"MDLZ\", \"Mondelez International, Inc.\", \"Mondelez\",\n",
    "    \"MELI\", \"MercadoLibre, Inc.\", \"MercadoLibre\",\n",
    "    \"MNST\", \"Monster Beverage Corporation\", \"Monster Beverage\",\n",
    "    \"MRVL\", \"Marvell Technology, Inc.\", \"Marvell Technology\",\n",
    "    \"MSTR\", \"MicroStrategy Incorporated\",\n",
    "    \"MU\", \"Micron Technology, Inc.\", \"Micron\",\n",
    "    \"NXPI\", \"NXP Semiconductors N.V.\", \"NXP Semiconductors\",\n",
    "    \"ODFL\", \"Old Dominion Freight Line, Inc.\", \"Old Dominion Freight Line\",\n",
    "    \"ON\", \"ON Semiconductor Corporation\",\n",
    "    \"ORLY\", \"O'Reilly Automotive, Inc.\", \"O'Reilly Automotive\",\n",
    "    \"PANW\", \"Palo Alto Networks, Inc.\", \"Palo Alto Networks\",\n",
    "    \"PAYX\", \"Paychex, Inc.\", \"Paychex\",\n",
    "    \"PCAR\", \"PACCAR Inc.\", \"PACCAR\",\n",
    "    \"PDD\", \"PDD Holdings Inc.\", \"PDD Holdings\",\n",
    "    \"PLTR\", \"Palantir Technologies Inc.\", \"Palantir Technologies\",\n",
    "    \"PYPL\", \"PayPal Holdings, Inc.\", \"PayPal\",\n",
    "    \"QCOM\", \"QUALCOMM Incorporated\",\n",
    "    \"REGN\", \"Regeneron Pharmaceuticals, Inc.\", \"Regeneron Pharmaceuticals\",\n",
    "    \"ROP\", \"Roper Technologies, Inc.\", \"Roper Technologies\",\n",
    "    \"ROST\", \"Ross Stores, Inc.\", \"Ross Stores\",\n",
    "    \"SBUX\", \"Starbucks Corporation\", \"Starbucks\",\n",
    "    \"SHOP\", \"Shopify Inc.\", \"Shopify\",\n",
    "    \"SNPS\", \"Synopsys, Inc.\", \"Synopsys\",\n",
    "    \"TEAM\", \"Atlassian Corporation Plc\", \"Atlassian\",\n",
    "    \"TMUS\", \"T-Mobile US, Inc.\", \"T-Mobile US\",\n",
    "    \"TTD\", \"The Trade Desk, Inc.\", \"The Trade Desk\",\n",
    "    \"TTWO\", \"Take-Two Interactive Software, Inc.\", \"Take-Two Interactive\",\n",
    "    \"TXN\", \"Texas Instruments Incorporated\", \"Texas Instruments\",\n",
    "    \"VRSK\", \"Verisk Analytics, Inc.\", \"Verisk Analytics\",\n",
    "    \"VRTX\", \"Vertex Pharmaceuticals Incorporated\", \"Vertex Pharmaceuticals\",\n",
    "    \"WBD\", \"Warner Bros. Discovery, Inc.\", \"Warner Bros. Discovery\",\n",
    "    \"WDAY\", \"Workday, Inc.\", \"Workday\",\n",
    "    \"XEL\", \"Xcel Energy Inc.\", \"Xcel Energy\",\n",
    "    \"ZS\", \"Zscaler, Inc.\", \"Zscaler\",\n",
    "]\n",
    "\n",
    "'''\n",
    "# Finance-focused domains\n",
    "trusted_financial_domains = [\n",
    "    'reuters.com', 'bloomberg.com', 'cnbc.com', 'ft.com', 'wsj.com',\n",
    "    'forbes.com', 'marketwatch.com', 'seekingalpha.com', 'investing.com'\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a96e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Domain filter\n",
    "'''\n",
    "def is_financial_domain(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        return any(fin_domain in domain for fin_domain in trusted_financial_domains)\n",
    "    except:\n",
    "        return False\n",
    "'''\n",
    "# Keyword match in title\n",
    "def title_mentions_ndx(text, keywords):\n",
    "    return any(k.lower() in text.lower() for k in keywords)\n",
    "\n",
    "# Named Entity Recognition for ORG/PRODUCT\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in (\"ORG\", \"PRODUCT\")]\n",
    "\n",
    "# Sentiment inference\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = finbert(text[:512])[0]\n",
    "        return {'label': result['label'], 'score': result['score']}\n",
    "    except:\n",
    "        return {'label': 'NEUTRAL', 'score': 0.0}\n",
    "\n",
    "# Main GDELT processor\n",
    "def process_gdelt_file(gdelt_file, max_articles=200):\n",
    "    df = pd.read_csv(gdelt_file, sep=',', header=None, low_memory=False)\n",
    "    df.columns = [f\"col_{i}\" for i in range(len(df.columns))]\n",
    "\n",
    "    # Target columns for filtering (GDELT \"themes\"/\"categories\" start around col_16 in V2)\n",
    "    target_cols = ['col_16', 'col_17', 'col_18', 'col_19']\n",
    "    keywords = {'BUS', 'BUSINESS', 'COMPANY'}\n",
    "\n",
    "    # Filter rows containing business keywords in target columns\n",
    "    df_filtered = df[df[target_cols].apply(lambda row: any(word in str(cell) for word in keywords for cell in row), axis=1)]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"⚠️ No relevant business-related rows found in {gdelt_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_filtered['url'] = df_filtered[f\"col_{len(df_filtered.columns) - 1}\"]\n",
    "\n",
    "    try:\n",
    "        df_filtered['date'] = pd.to_datetime(df_filtered['col_1'], format='%Y%m%d', errors='coerce')\n",
    "    except:\n",
    "        df_filtered['date'] = pd.NaT\n",
    "\n",
    "    print(f\"✅ Loaded {len(df_filtered)} filtered rows from {gdelt_file}\")\n",
    "\n",
    "    sentiment_results = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    for _, row in tqdm(df_filtered.iterrows(), total=min(len(df_filtered), max_articles * 2)):\n",
    "        url = row['url']\n",
    "        if url in seen_urls or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "            continue\n",
    "        seen_urls.add(url)\n",
    "\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            title = article.title or \"\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Relevance check\n",
    "        if title_mentions_ndx(title, nasdaq_keywords):\n",
    "            relevant = True\n",
    "        else:\n",
    "            org_entities = extract_entities(title)\n",
    "            relevant = any(k.lower() in ' '.join(org_entities).lower() for k in nasdaq_keywords)\n",
    "\n",
    "        if not relevant:\n",
    "            continue\n",
    "\n",
    "        article_text = article.text.strip()\n",
    "        if len(article_text) < 100:\n",
    "            continue\n",
    "\n",
    "        sentiment = analyze_sentiment(article_text)\n",
    "        sentiment_results.append({\n",
    "            'date': row['date'],\n",
    "            'url': url,\n",
    "            'label': sentiment['label'].upper().strip(),\n",
    "            'score': sentiment['score'],\n",
    "            'text': article_text[:200]\n",
    "        })\n",
    "\n",
    "        if len(sentiment_results) >= max_articles:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(sentiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72dbdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_daily_sentiment(df_sentiments):\n",
    "    label_map = {'POSITIVE': 1, 'NEGATIVE': -1, 'NEUTRAL': 0}\n",
    "    df_sentiments['label_num'] = df_sentiments['label'].map(label_map)\n",
    "\n",
    "    agg = df_sentiments.groupby('date').agg({\n",
    "        'label_num': 'mean',\n",
    "        'score': 'mean',\n",
    "        'url': 'count'\n",
    "    }).rename(columns={\n",
    "        'label_num': 'avg_sentiment',\n",
    "        'score': 'avg_score',\n",
    "        'url': 'article_count'\n",
    "    })\n",
    "\n",
    "    return agg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec4f8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbfca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Processing: 20250101_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['url'] = df_filtered[f\"col_{len(df_filtered.columns) - 1}\"]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['date'] = pd.to_datetime(df_filtered['col_1'], format='%Y%m%d', errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 641 filtered rows from ../data/gdelt_exports_cache_concat\\20250101_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 172/600 [01:49<05:59,  1.19it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "641it [08:09,  1.31it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Processing: 20250102_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['url'] = df_filtered[f\"col_{len(df_filtered.columns) - 1}\"]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['date'] = pd.to_datetime(df_filtered['col_1'], format='%Y%m%d', errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 978 filtered rows from ../data/gdelt_exports_cache_concat\\20250102_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "978it [13:19,  1.22it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Processing: 20250103_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['url'] = df_filtered[f\"col_{len(df_filtered.columns) - 1}\"]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['date'] = pd.to_datetime(df_filtered['col_1'], format='%Y%m%d', errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1208 filtered rows from ../data/gdelt_exports_cache_concat\\20250103_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [14:51,  1.36it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Processing: 20250104_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['url'] = df_filtered[f\"col_{len(df_filtered.columns) - 1}\"]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_17480\\1628000028.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['date'] = pd.to_datetime(df_filtered['col_1'], format='%Y%m%d', errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 619 filtered rows from ../data/gdelt_exports_cache_concat\\20250104_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 97/600 [01:08<05:23,  1.55it/s]"
     ]
    }
   ],
   "source": [
    "# 🔁 Loop over all *_combined.csv files\n",
    "combined_files = sorted([\n",
    "    f for f in os.listdir(input_dir) if f.endswith('_combined.csv')\n",
    "])\n",
    "\n",
    "for file in combined_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    date_str = file[:8]  # Extract YYYYMMDD\n",
    "\n",
    "    print(f\"\\n📅 Processing: {file}\")\n",
    "\n",
    "    sentiment_df = process_gdelt_file(file_path, max_articles=300)\n",
    "    sentiment_out_path = os.path.join(output_dir, f\"{date_str}_sentiments.csv\")\n",
    "    sentiment_df.to_csv(sentiment_out_path, index=False)\n",
    "\n",
    "    daily_score_df = aggregate_daily_sentiment(sentiment_df)\n",
    "    daily_score_path = os.path.join(output_dir, f\"{date_str}_daily_score.csv\")\n",
    "    daily_score_df.to_csv(daily_score_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
